---
title: Roberta base chinese extractive qa
date: 2022/5/29
categories: 科研
tags: [NLP, QA模型 , 抽取式 , 中文 ]
---

<meta name="referrer" content="no-referrer" />

抽取式中文QA模型baseline

<!--more-->



# 模型使用过程

本次需要做一个抽取式中文阅读理解的baseline，在hugging face上找到了如下模型：[ roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)，目前而言当作baseline没有问题。

此模型使用[UER-py](https://github.com/dbiir/UER-py)辅助微调，在预训练模型 [chinese_roberta_L-12_H-768](https://huggingface.co/uer/chinese_roberta_L-12_H-768)（L为layers，H为hidden sizes）的基础上对序列长度为512的三个 epoch进行微调。在每个epoch结束时，当开发集的最佳性能达到时，模型被保存。代码如下：

```python
python3 run_cmrc.py --pretrained_model_path models/cluecorpussmall_roberta_base_seq512_model.bin-250000 \
                    --vocab_path models/google_zh_vocab.txt \
                    --train_path extractive_qa.json \
                    --dev_path datasets/cmrc2018/dev.json \
                    --output_model_path models/extractive_qa_model.bin \
                    --learning_rate 3e-5 --epochs_num 3 --batch_size 32 --seq_length 512
```

训练数据为 [cmrc2018](https://github.com/ymcui/cmrc2018)，[webqa](https://spaces.ac.cn/archives/4338)，[laisi](https://www.kesci.com/home/competition/5d142d8cbb14e6002c04e14a/content/0)。

代码如下：

```python
pip install transformers
pip install pytorch
from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline
model = AutoModelForQuestionAnswering.from_pretrained('roberta-base-chinese-extractive-qa')
tokenizer = AutoTokenizer.from_pretrained('roberta-base-chinese-extractive-qa')
QA = pipeline('question-answering', model=model, tokenizer=tokenizer)
QA_input = {'question': "著名诗歌《假如生活欺骗了你》的作者是",'context': "普希金从那里学习人民的语言，吸取了许多有益的养料，这一切对普希金后来的创作产生了很大的影响。这两年里，普希金创作了不少优秀的作品，如《囚徒》、《致大海》、《致凯恩》和《假如生活欺骗了你》等几十首抒情诗，叙事诗《努林伯爵》，历史剧《鲍里斯·戈都诺夫》，以及《叶甫盖尼·奥涅金》前六章"}
QA(QA_input)
```

结果为：

```python
{'score': 0.9766426682472229, 'start': 0, 'end': 3, 'answer': '普希金'}
```

# roberta 论文解读

论文：[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

## Abstract

1. 预训练模型在语言模型中起到了很大的作用，但是训练成本很高。并且很多预训练模型都是使用不同大小的私有数据集进行训练，**超参数的选择**也会直接对结果产生影响。
2. Bert模型没有充分得到训练，但是仍然可以超过其他模型的效果。

## 1.Introduction

提出了一个改进的BERT模型训练配方，我们称之为**RoBERTa**（A Robustly Optimized BERT），它可以匹配或超过post-BERT模型的性能。我们的修改如下：

1. 在更多的数据上用更大的批次训练模型
2. 移除了Bert模型中next sentence prediction objective
3. 在更长的序列上进行训练
4. 动态地改变应用于训练数据的掩蔽模式

还收集了一个新的大型数据集（CC-NEWS），其规模与其他私人使用的数据集相当，以更好地控制训练集的规模效应。

在GLUE和SQuAD上取得较好成果。

本文主要贡献有：

1. 提出了一套重要的BERT设计选择和训练策略，并引入了能够提高下游任务性能的备选方案
2. 使用了一个新的数据集CCNEWS，并确认使用更多的数据进行预训练可以进一步提高下游任务的性能
3. 训练改进表明，在正确的设计选择下，屏蔽语言模型预训练与所有其他最近发表的方法都更具有竞争力。

## 2.Background

### 2.1 Setup

BERT将两个片段的连接作为输入：$x_1,...,x_N$和$y_1,...,y_M$，段落通常由一个以上的自然句组成。这两个片段作为一个单一的输入序列呈现给BERT，并以特殊的标记为它们划界：$[CLS],x_1,...,x_N,[SEP],y_1,...,y_M,[EOS]$，$M+N < T$，T是训练期间的最大序列长度。

该模型首先在一个大型的无标签文本语料库中进行预训练，随后使用终端任务的标签数据进行微调。

### 2.2 Architecture

使用 L layers. Each block uses A self-attention heads and hidden dimension H的transformer结构

### 2.3 Training Objectives

在预训练期间，BERT使用了两个方法：**遮蔽语言建模和下一句话预测**。

#### Masked Language Model (MLM)

在输入序列中选择一个随机的标记token，用特殊的标记[MASK]代替。MLM的目标是预测被屏蔽token的交叉熵损失。BERT均匀地选择15%的输入token进行可能的替换。在所选的token中，80%被替换为[MASK]，10%保持不变，10%被随机选择的词汇标记所替换。在最初的实现中，随机屏蔽和替换在开始时进行一次，并在训练过程中保存，尽管在实践中，数据是重复的，所以每个训练句子的屏蔽并不总是相同的。

#### Next Sentence Prediction (NSP)

NSP是一种二元分类损失，用于预测原始文本中的两个片段是否相互跟随。正面例子是通过从文本语料库中抽取连续的句子来创建的。负面的例子是通过将不同文件中的片段配对来创建的。正面和负面的例子是以相同的概率取样的。NSP目标是为了提高下游任务的性能，这需要推理成对的句子之间的关系。

### 2.4 Optimization

BERT模型使用的是Adam优化器，超参数$\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = 1e-6，权重衰减参数为0.01，学习率在前10000步逐渐增加到1e-4，然后线性衰减，dropout参数为0.1，batch=256，token_length=512。

### 2.5 Data

在BOOKCORPUS和英语WIKIPEDIA的组合上进行训练

## 3 Experimental Setup

1. 实验的超参数基本沿用了Bert模型的参数
2. 文章发现模型结果对于Adam优化器的参数很敏感。经过对比发现当训练更大的batch时，设置$\beta_2$ = 0.98更为稳定。
3. 原始Bert模型中，在训练过程的前90%的时候，采用了随机注入短序列的方法。本文的实验中全部使用的是足够长度的序列。

## 4 Training Procedure Analysis

### 4.1 Static vs. Dynamic Masking

最原始的Bert模型采用的是静态mask，也就是选取序列中的15%个token进行mask，然后在训练过程中保持不变。RoBERTa模型**为了避免每次都使用相同的mask，采用了改进版本的静态mask**。具体上，同一个训练数据被重复10次，10次都使用不同的mask，然后训练40个epochs。

此外，作者还提出了动态mask，每次想模型输入数据，都使用不同的mask。

结果如图：

![image-20220529200908276](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205292009316.png)

采用动态mask效果会稍微好一点

### 4.2 Model Input Format and Next Sentence Prediction

在原始的Bert模型中，原文作者认为Next Sentence Prediction对于模型起到很重要的作用。**但是最近的研究表明，NSP或许是可以删去的。**为了比较区别，做了如下的对照试验：

1. SEGMENT-PAIR+NSP：和原始的bert模型一致，**有NSP损失**。每个输入都有一对片段，每个片段可以包含多个自然句子，但总的综合长度必须小于512个标记。（**片段级别**）
2. SENTENCE-PAIR+NSP：每个输入包含一对自然句子，可以从一个文件的连续部分取样，也可以从不同的文件中取样。由于这些输入明显短于512个标记，我们增加了批量大小，使标记的总数保持与SEGMENT-PAIR+NSP相似。**保留了NSP的损失**。（**句子级别**）
3. FULL-SENTENCES：每个输入都是由从一个或多个文件中连续取样的完整句子组成，因此总长度最多为512个符号。输入的句子可以跨越文件的边界。当我们到达一个文件的末尾时，我们开始从下一个文件中取样，并在文件之间增加一个额外的分隔符。**去除NSP的损失**。**存在跨文档的可能性**
4. DOC-SENTENCES：输入的构造与FULL-SENTENCES类似，只是它们不能跨越文档的边界。在文档末尾附近取样的输入可能短于512个标记，所以我们在这些情况下动态地增加批处理量，以达到与FULLSENTENCES相似的总标记数。**去除NSP的损失**，**不存在跨文档的可能性**。

结果如图：

![image-20220529201512128](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205292015268.png)

**发现移除NSP会比原始Bert模型稍微好一点**。FULL-SENTENCES和DOC-SENTENCES则是差不多效果

### 4.3 Training with large batches

过去在神经机器翻译方面的工作表明，当学习率适当提高时，使用**非常大的小批量**进行训练可以提高优化速度和结束任务的性能。最近的研究表明，**BERT也可以接受大批量培训**原始的Bert模型使用batchsize=256的模型训练1M个step，这等价于batchsize=2K的模型训练125K个step，等价于batchsize=8K训练31K个step。结果如图：

![image-20220529201700824](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205292017892.png)

大批量的训练提高了屏蔽语言建模目标的复杂性，以及最终任务的准确性。通过分布式数据并行训练，大批量也更容易并行化，在随后的实验中，使用8K序列的批量进行训练。

### 4.4 Text Encoding

Byte-Pair Encoding（BPE）采用的是字符级和单词级的混合特征，该编码方案可以处理自然语言语料库中常见的大量词汇。BPE不依赖于完整的单词，而是依赖于子词(sub-word)单元。两种BPE实现方式：

+ 基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。
+ 基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。

本文采用byte-level BPE，这样能使模型获得更多可学习的参数。

## 5 RoBERTa

**RoBERTa模型是整合上述改进而提出来的模型。**

**RoBERTa模型采用动态masking、FULL-SENTENCES without NSP、更多的小批量、byte-level BPE。**

首先按照BERT-Large架构（L=24，H=1024，A=16）对RoBERTa进行训练。用BOOKCORPUS和WIKIPEDIA数据集进行了100K步预训练。使用1024块V100 GPU对模型进行了大约一天的预训练。

RoBERTa的开发集结果如下图，因为预先训练了更多数据（16GB→160GB的文本）和预训练更长时间（100K→300K→500K步），每行累积上述行的改进。RoBERTa与BERTLARGE的架构和训练目标一致。

![image-20220530103938275](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205301039368.png)

在GLUE上的结果：

![image-20220530105614211](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205301056226.png)

在SQuAD上的结果：

![image-20220530105701438](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205301057217.png)

在RACE上的结果：

![image-20220530105714448](https://raw.githubusercontent.com/Rien190/ImgURL/master/img/202205301057180.png)

## 6 Related Work

预训练方法的设计有不同的训练目标，包括语言建模、机器翻译和屏蔽语言模型。最近的许多论文都采用了为每个终端任务微调模型的基本配方，并以某种变体的遮蔽语言模型目标进行预训练。然而，较新的方法通过多任务微调、纳入实体嵌入、跨度预测和自回归预训练的多种变体来提高性能。通过在更多的数据上训练更大的模型，通常也能提高性能。我们的目标是复制、简化和更好地调整BERT的训练，作为一个参考点，以更好地了解所有这些方法的相对性能。

## 7 Conclusion

在预训练BERT模型时，本文仔细评估一些设计决策。通过**对模型进行更长时间的训练，在更多的数据上进行大批量的训练；取消下一句话的预测目标；在更长的序列上进行训练；以及动态地改变应用于训练数据的掩蔽模式可以大大改善性能**。上述预训练改进方案，即为本文所提出的RoBERTa，该方案在GLUE，RACE和SQuAD上实现了目前最好的结果。备注：在GLUE上没有进行多任务微调，在SQuAD上没有使用附加数据。这些结果说明这些先前被忽视的设计决策的重要性，并表明BERT的预训练目标与最近提出的替代方案相比仍然具有竞争性。







# reference

[ roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)

[UER-py](https://github.com/dbiir/UER-py)

 [chinese_roberta_L-12_H-768](https://huggingface.co/uer/chinese_roberta_L-12_H-768)

 [cmrc2018](https://github.com/ymcui/cmrc2018)

[webqa](https://spaces.ac.cn/archives/4338)

[laisi](https://www.kesci.com/home/competition/5d142d8cbb14e6002c04e14a/content/0)

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)



